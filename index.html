---
layout: default
title: Wolfe &bull; expressive, extensible and efficient ML
overview: true
---

<section class="intro">
    <div class="grid">
        <div class="unit whole center-on-mobiles">
            <p class="first">Declarative Machine Learning</p>
        </div>
    </div>
</section>
<section class="features">
    <div class="grid">
        <div class="unit two-thirds">
            <h2>Expressive, Extensible and Concise</h2>

            <p>
                In Wolfe you use <a href="http://www.scala-lang.org/">Scala</a> to define machine learning models in terms of real valued functions
                such as densities, energy functions and empirical losses&mdash;almost as concise as in a machine learning paper.
                This paradigm enables conditional random fields, generative models, markov logic networks, matrix factorization and more.
                See examples below and follow our <a href="http://moro.wolfe.ml:9000/doc/wolfe-static/wolfe/docs/gettingstarted/01_welcome">interactive tutorials</a>.
            </p>

            <div class="myslider flexslider">
                <ul class="slides">
                    <li>
                        <div class="code-example">
<pre class="prettyprint lang-scala">
//transition and observation features of a <a href="http://en.wikipedia.org/wiki/Conditional_random_field">linear chain CRF</a>
def f(s:Sentence) = {
  val n = s.words.size
  sum (0 until n) { i=> oneHot(s.words(i)->s.tags(i))} +
  sum (0 until n-1) { i=> oneHot(s.tags(i)->s.tags(i+1))}}

//the corresponding linear model
def s(w:Vector)(s:Sentence) = w dot f(s)
</pre>
                        </div>
                    </li>
                    <li>
                        <div class="code-example">
<pre class="prettyprint lang-scala">
// MAP inference
def h(w:Vector)(x:Sentence):Sentence =
  argmax(sentences st (obs(_)==obs(x))){ d=>(w)(d) }

// Loss over training data
def loss(data:Seq[Sentence])(w:Vector):Double =
  sum(data){ d=>s(w)(h(w)(d))-s(w)(d) }

// Parameter estimation
def learn(data:Seq[Sentence]) = argmin(vectors){w=> loss(data)(w)}
</pre>
                        </div>
                    </li>
                    <li>
                        <div class="code-example">
<pre class="prettyprint lang-scala">
// latent factorization and neighborhood model
def s(w:Vector)(d:UserItem) =
  sum(0 until k){ i => w(u.item->i)*w(u.user->i) } +
  sum(u.user.items){ i => w(i->u.item) }

// training loss over observed cells
def loss(data:Seq[UserItem])(w:Vector) =
  sum(data){ d => pow2(d.rating - s(w)(d))}
</pre>
                        </div>
                    </li>

                </ul>
            </div>
            <!--<a href="/docs/usage/">Interactive tutorials&rarr;</a>-->
        </div>
        <div class="unit one-third">
            <h2>Efficient</h2>

            <p>Wolfe compiles the declarative Scala code into highly optimized
               inference and learning routines that maintain or approximate the semantics of the original code.
               This allows you to focus on modelling while Wolfe is doing the heavy lifting.
            </p>
            <p>There is a limit to automatic optimization. Wolfe therefore provides hooks to inject
                procedural background knowledge such as optimization
                subroutines for specific types of sub-functions.
                <!--<a href="/docs/templates/">Clone Wolfe</a>-->
                <!--to help us develop a growing ecosystem of such sub-function and subroutine pairs.-->
            </p>
        </div>
        <div class="clear"></div>
    </div>
</section>

